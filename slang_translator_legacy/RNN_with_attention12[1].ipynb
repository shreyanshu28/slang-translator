{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-02-08T14:20:16.072767Z",
     "iopub.status.busy": "2023-02-08T14:20:16.071996Z",
     "iopub.status.idle": "2023-02-08T14:20:22.585257Z",
     "shell.execute_reply": "2023-02-08T14:20:22.584139Z",
     "shell.execute_reply.started": "2023-02-08T14:20:16.072738Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!pip3 install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7_vVuq4Y68YP"
   },
   "outputs": [],
   "source": [
    "# NOTE: The order of train set and test set should be reversed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:17:56.386977Z",
     "iopub.status.busy": "2023-02-11T12:17:56.386485Z",
     "iopub.status.idle": "2023-02-11T12:17:59.524925Z",
     "shell.execute_reply": "2023-02-11T12:17:59.523906Z",
     "shell.execute_reply.started": "2023-02-11T12:17:56.386900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# suppress info and warnings outputted by tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# enable memory growth for gpu devices\n",
    "# source: https://stackoverflow.com/a/55541385/8849692\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:21:34.929172Z",
     "iopub.status.busy": "2023-02-11T12:21:34.928786Z",
     "iopub.status.idle": "2023-02-11T12:21:35.408451Z",
     "shell.execute_reply": "2023-02-11T12:21:35.407377Z",
     "shell.execute_reply.started": "2023-02-11T12:21:34.929133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 11 18:28:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   39C    P8     2W /  N/A |     44MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     11376    C+G   ...ser\\Application\\brave.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:06:11.780444Z",
     "iopub.status.busy": "2023-02-11T12:06:11.779924Z",
     "iopub.status.idle": "2023-02-11T12:07:02.386425Z",
     "shell.execute_reply": "2023-02-11T12:07:02.385629Z",
     "shell.execute_reply.started": "2023-02-11T12:06:11.780418Z"
    },
    "id": "LO3QoKy_6pbp",
    "outputId": "cedeb83e-0aa6-4c5f-cf6e-fcb8d35710e3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text>=2.10 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-text>=2.10) (0.12.0)\n",
      "Requirement already satisfied: tensorflow<2.11,>=2.10.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-text>=2.10) (2.10.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (15.0.6.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (23.1.21)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.23.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.51.1)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (0.30.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.10.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.10.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.16.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (3.19.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (4.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (21.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text>=2.10) (3.2.2)\n",
      "Requirement already satisfied: einops in c:\\users\\shrey\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tensorflow-text>=2.10\"\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:08:00.843451Z",
     "iopub.status.busy": "2023-02-11T12:08:00.842791Z",
     "iopub.status.idle": "2023-02-11T12:08:00.849100Z",
     "shell.execute_reply": "2023-02-11T12:08:00.847988Z",
     "shell.execute_reply.started": "2023-02-11T12:08:00.843416Z"
    },
    "id": "xEsLcHAc75iS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:08:04.893638Z",
     "iopub.status.busy": "2023-02-11T12:08:04.893308Z",
     "iopub.status.idle": "2023-02-11T12:08:04.899970Z",
     "shell.execute_reply": "2023-02-11T12:08:04.899257Z",
     "shell.execute_reply.started": "2023-02-11T12:08:04.893615Z"
    },
    "id": "HYfU3X3L77_Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return\n",
    "\n",
    "    parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "    for name, new_dim in parsed.items():\n",
    "      old_dim = self.shapes.get(name, None)\n",
    "      \n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:08:06.508123Z",
     "iopub.status.busy": "2023-02-11T12:08:06.507616Z",
     "iopub.status.idle": "2023-02-11T12:08:06.515696Z",
     "shell.execute_reply": "2023-02-11T12:08:06.514623Z",
     "shell.execute_reply.started": "2023-02-11T12:08:06.508087Z"
    },
    "id": "QQ-xnPdj7-yr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path1, path2):\n",
    "  # text1 = path1.read_text(encoding='utf-8')\n",
    "  # text2 = path2.read_text(encoding='utf-8')\n",
    "  # with open(path1, \"r\", encoding=\"utf-8\") as f:\n",
    "  #   lines1 = f.readlines()\n",
    "  #   # f=open(\"filename.txt\",\"r\",encoding='utf-8')\n",
    "  #   # lines1 = text1.splitlines()\n",
    "  # with open(path2) as f:\n",
    "  #   lines2 = f.readlines()\n",
    "  #   # lines2 = text2.splitlines(encoding='utf-8')\n",
    "  # # pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "  # target = np.array([x.strip(\"\\n\") for x in lines1])\n",
    "  # context = np.array([x.strip(\"\\n\") for x in lines2])\n",
    "\n",
    "  \n",
    "\n",
    "  return np.array(open(path1, encoding='utf-8').\\\n",
    "         read().strip().split('\\n')),np.array(open(path2, encoding='utf-8').\\\n",
    "         read().strip().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:08:08.274544Z",
     "iopub.status.busy": "2023-02-11T12:08:08.274196Z",
     "iopub.status.idle": "2023-02-11T12:08:08.967104Z",
     "shell.execute_reply": "2023-02-11T12:08:08.966218Z",
     "shell.execute_reply.started": "2023-02-11T12:08:08.274518Z"
    },
    "id": "bodBlUhQ8P-q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_raw, context_raw = load_data(\"informal\", \"formal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASbdtc7DUcCG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:08:10.667223Z",
     "iopub.status.busy": "2023-02-11T12:08:10.666883Z",
     "iopub.status.idle": "2023-02-11T12:08:10.672699Z",
     "shell.execute_reply": "2023-02-11T12:08:10.671517Z",
     "shell.execute_reply.started": "2023-02-11T12:08:10.667199Z"
    },
    "id": "r1GSSNxA8XVm",
    "outputId": "7f7b795d-0cd4-4d3f-a4bd-e8fa61953e70",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do have it. Please send me a message and I will send it to you.\n",
      "I have it if you would like it... message me and I'll gladly send it to you!!\n"
     ]
    }
   ],
   "source": [
    "print(context_raw[-1])\n",
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:08:13.242840Z",
     "iopub.status.busy": "2023-02-11T12:08:13.242453Z",
     "iopub.status.idle": "2023-02-11T12:08:15.621981Z",
     "shell.execute_reply": "2023-02-11T12:08:15.621010Z",
     "shell.execute_reply.started": "2023-02-11T12:08:13.242813Z"
    },
    "id": "PsiwbM9B8X4E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:10:45.273467Z",
     "iopub.status.busy": "2023-02-11T12:10:45.272844Z",
     "iopub.status.idle": "2023-02-11T12:10:45.422016Z",
     "shell.execute_reply": "2023-02-11T12:10:45.421263Z",
     "shell.execute_reply.started": "2023-02-11T12:10:45.273440Z"
    },
    "id": "rAuGZslK8X5U",
    "outputId": "1951bf5e-eeaf-405c-953a-a9b20e19a920",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Yes, we need another idol please.' b'It is education.'\n",
      " b'Be sure to review the instructions your computer may be the problem.'\n",
      " b'How long ago? A decade?' b\"Batman is great. I wouldn't of had guessed.\"], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'yes, please we need another idol!...'\n",
      " b'Oh Crap, Not Another Learning Experience!'\n",
      " b'check the instructions... it might be your computer...'\n",
      " b'or as in like a decade ago?'\n",
      " b'/&#92; Batman is Awesome /&#92; , i would have never guessed.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "  print(example_context_strings[:5])\n",
    "  print()\n",
    "  print(example_target_strings[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:08:18.897385Z",
     "iopub.status.busy": "2023-02-11T12:08:18.896979Z",
     "iopub.status.idle": "2023-02-11T12:08:18.906595Z",
     "shell.execute_reply": "2023-02-11T12:08:18.905560Z",
     "shell.execute_reply.started": "2023-02-11T12:08:18.897357Z"
    },
    "id": "3-6aN9fn84Y7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def custom_standardizations(input_data):\n",
    "  # Split accented characters.\n",
    "  text = tf_text.normalize_utf8(input_data, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  return tf.strings.join(['[START]', text, '[END]'], separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T11:53:03.406382Z",
     "iopub.status.busy": "2023-02-11T11:53:03.405951Z",
     "iopub.status.idle": "2023-02-11T11:53:03.422647Z",
     "shell.execute_reply": "2023-02-11T11:53:03.421730Z",
     "shell.execute_reply.started": "2023-02-11T11:53:03.406354Z"
    },
    "id": "8sh9UxZy9Ykp",
    "outputId": "76df0b2a-be39-4730-944e-8b0011278a4a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'How old are you!?!!'\n",
      "b'How old are you!?!!'\n",
      "How old are you!?!!\n",
      "[START] how old are you !  ?  !  ! [END]\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('How old are you!?!!')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())\n",
    "print(example_text.numpy().decode())\n",
    "print(custom_standardizations(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:08:23.440824Z",
     "iopub.status.busy": "2023-02-11T12:08:23.440409Z",
     "iopub.status.idle": "2023-02-11T12:08:23.465166Z",
     "shell.execute_reply": "2023-02-11T12:08:23.464150Z",
     "shell.execute_reply.started": "2023-02-11T12:08:23.440797Z"
    },
    "id": "AxNYdJUF9kP2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 15000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardizations,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardizations,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:10:00.405069Z",
     "iopub.status.busy": "2023-02-11T12:10:00.403672Z",
     "iopub.status.idle": "2023-02-11T12:10:02.486086Z",
     "shell.execute_reply": "2023-02-11T12:10:02.485258Z",
     "shell.execute_reply.started": "2023-02-11T12:10:00.405010Z"
    },
    "id": "ca6Qc5sZ-QbP",
    "outputId": "1490b54b-cbab-4135-cfd1-512918ba91d4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '.', '[START]', '[END]', 'i', ',', 'the', 'is', 'to']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:09:55.031726Z",
     "iopub.status.busy": "2023-02-11T12:09:55.031352Z",
     "iopub.status.idle": "2023-02-11T12:09:57.502845Z",
     "shell.execute_reply": "2023-02-11T12:09:57.502098Z",
     "shell.execute_reply.started": "2023-02-11T12:09:55.031702Z"
    },
    "id": "ocqDBpVa-Zqz",
    "outputId": "65145b46-2996-4031-feab-f4ea5a4b4051",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '.', '[START]', '[END]', '!', 'i', ',', 'the', 'a']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:10:48.315898Z",
     "iopub.status.busy": "2023-02-11T12:10:48.315019Z",
     "iopub.status.idle": "2023-02-11T12:10:48.376849Z",
     "shell.execute_reply": "2023-02-11T12:10:48.376242Z",
     "shell.execute_reply.started": "2023-02-11T12:10:48.315871Z"
    },
    "id": "njY7LABd-Zrr",
    "outputId": "f12a7abc-bb88-4328-a779-7214959445f4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[3, 80, 6, 114, 135, 270, 480, 110, 2, 4], [3, 11, 8, 3575, 2, 4],\n",
       " [3, 27, 104, 9, 3423, 7, 3852, 38, 506, 133, 27, 7, 558, 2, 4]]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = context_text_processor(example_context_strings)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:10:50.191699Z",
     "iopub.status.busy": "2023-02-11T12:10:50.190532Z",
     "iopub.status.idle": "2023-02-11T12:10:50.245492Z",
     "shell.execute_reply": "2023-02-11T12:10:50.244870Z",
     "shell.execute_reply.started": "2023-02-11T12:10:50.191661Z"
    },
    "id": "3stX1tEh-oSO",
    "outputId": "b583db7d-4553-4591-ad98-fa7b4fdab6c8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] yes , we need another idol please . [END]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:10:51.934441Z",
     "iopub.status.busy": "2023-02-11T12:10:51.933438Z",
     "iopub.status.idle": "2023-02-11T12:10:52.174263Z",
     "shell.execute_reply": "2023-02-11T12:10:52.173566Z",
     "shell.execute_reply.started": "2023-02-11T12:10:51.934414Z"
    },
    "id": "Fw0JBw2A-o5G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "  context = context_text_processor(context).to_tensor()\n",
    "  target = target_text_processor(target)\n",
    "  targ_in = target[:,:-1].to_tensor()\n",
    "  targ_out = target[:,1:].to_tensor()\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:10:54.098182Z",
     "iopub.status.busy": "2023-02-11T12:10:54.097022Z",
     "iopub.status.idle": "2023-02-11T12:10:54.277518Z",
     "shell.execute_reply": "2023-02-11T12:10:54.276916Z",
     "shell.execute_reply.started": "2023-02-11T12:10:54.098144Z"
    },
    "id": "TTcF8UUN-ycY",
    "outputId": "3a2ccedb-7eb1-4610-92ae-1651dfdce840",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3  713   11  209    9    7 1414    2    4    0]\n",
      "\n",
      "[   3  636   11   45    8  104  169   12    8 1301]\n",
      "[ 636   11   45    8  104  169   12    8 1301 1156]\n"
     ]
    }
   ],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "  print(ex_context_tok[0, :10].numpy()) \n",
    "  print()\n",
    "  print(ex_tar_in[0, :10].numpy()) \n",
    "  print(ex_tar_out[0, :10].numpy()) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:10:56.097527Z",
     "iopub.status.busy": "2023-02-11T12:10:56.096466Z",
     "iopub.status.idle": "2023-02-11T12:10:56.101174Z",
     "shell.execute_reply": "2023-02-11T12:10:56.100310Z",
     "shell.execute_reply.started": "2023-02-11T12:10:56.097499Z"
    },
    "id": "_uRSZkrv-5m7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"encoder-decoder\"\"\"\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:02.721722Z",
     "iopub.status.busy": "2023-02-11T12:11:02.720808Z",
     "iopub.status.idle": "2023-02-11T12:11:02.729569Z",
     "shell.execute_reply": "2023-02-11T12:11:02.728598Z",
     "shell.execute_reply.started": "2023-02-11T12:11:02.721695Z"
    },
    "id": "m_qmIeW__I36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.units = units\n",
    "    \n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                               mask_zero=True)\n",
    "\n",
    "    # The RNN layer processes those vectors sequentially.\n",
    "    self.rnn = tf.keras.layers.Bidirectional(\n",
    "        merge_mode='sum',\n",
    "        layer=tf.keras.layers.GRU(units,\n",
    "                            # Return the sequence and state\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "  def call(self, x):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch s')\n",
    "\n",
    "    # 2. The embedding layer looks up the embedding vector for each token.\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "\n",
    "    # 3. The GRU processes the sequence of embeddings.\n",
    "    x = self.rnn(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "\n",
    "    # 4. Returns the new sequence of embeddings.\n",
    "    return x\n",
    "\n",
    "  def convert_input(self, texts):\n",
    "    texts = tf.convert_to_tensor(texts)\n",
    "    if len(texts.shape) == 0:\n",
    "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "    context = self.text_processor(texts).to_tensor()\n",
    "    context = self(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:05.203537Z",
     "iopub.status.busy": "2023-02-11T12:11:05.202524Z",
     "iopub.status.idle": "2023-02-11T12:11:05.222011Z",
     "shell.execute_reply": "2023-02-11T12:11:05.221252Z",
     "shell.execute_reply.started": "2023-02-11T12:11:05.203511Z"
    },
    "id": "U0WfiXf5_Trw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence.\n",
    "encoder = Encoder(context_text_processor, UNITS)\n",
    "#ex_context = encoder(ex_context_tok)\n",
    "\n",
    "#print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "#print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:07.365060Z",
     "iopub.status.busy": "2023-02-11T12:11:07.364196Z",
     "iopub.status.idle": "2023-02-11T12:11:07.371775Z",
     "shell.execute_reply": "2023-02-11T12:11:07.371038Z",
     "shell.execute_reply.started": "2023-02-11T12:11:07.365034Z"
    },
    "id": "uRlBsa8G_Z8B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=2, **kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, x, context):\n",
    "    shape_checker = ShapeChecker()\n",
    " \n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "    \n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(attn_scores, 'batch heads t s')\n",
    "    \n",
    "    # Cache the attention scores for plotting later.\n",
    "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "    shape_checker(attn_scores, 'batch t s')\n",
    "    self.last_attention_weights = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:09.111559Z",
     "iopub.status.busy": "2023-02-11T12:11:09.110382Z",
     "iopub.status.idle": "2023-02-11T12:11:09.123265Z",
     "shell.execute_reply": "2023-02-11T12:11:09.122237Z",
     "shell.execute_reply.started": "2023-02-11T12:11:09.111520Z"
    },
    "id": "8uPwxUTdSTsY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:12.966878Z",
     "iopub.status.busy": "2023-02-11T12:11:12.965814Z",
     "iopub.status.idle": "2023-02-11T12:11:12.975334Z",
     "shell.execute_reply": "2023-02-11T12:11:12.974609Z",
     "shell.execute_reply.started": "2023-02-11T12:11:12.966849Z"
    },
    "id": "-1AZrQAw_sZm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.word_to_id = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]')\n",
    "    self.id_to_word = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]',\n",
    "        invert=True)\n",
    "    self.start_token = self.word_to_id('[START]')\n",
    "    self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "    self.units = units\n",
    "\n",
    "\n",
    "    # 1. The embedding layer converts token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                               units, mask_zero=True)\n",
    "\n",
    "    # 2. The RNN keeps track of what's been generated so far.\n",
    "    self.rnn = tf.keras.layers.GRU(units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    #self.lstm = tf.keras.layers.LSTM(units,\n",
    "                                   #return_sequences=True,\n",
    "                                   #return_state=True,\n",
    "                                   #recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = CrossAttention(units)\n",
    "\n",
    "    # 4. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:14.475004Z",
     "iopub.status.busy": "2023-02-11T12:11:14.474379Z",
     "iopub.status.idle": "2023-02-11T12:11:14.481304Z",
     "shell.execute_reply": "2023-02-11T12:11:14.480341Z",
     "shell.execute_reply.started": "2023-02-11T12:11:14.474978Z"
    },
    "id": "ipz7NnZl__qz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "  shape_checker = ShapeChecker()\n",
    "  shape_checker(x, 'batch t')\n",
    "  shape_checker(context, 'batch s units')\n",
    "\n",
    "  # 1. Lookup the embeddings\n",
    "  x = self.embedding(x)\n",
    "  shape_checker(x, 'batch t units')\n",
    "\n",
    "  # 2. Process the target sequence.\n",
    "  x, state = self.rnn(x, initial_state=state)\n",
    "  shape_checker(x, 'batch t units')\n",
    "\n",
    "  # 3. Use the RNN output as the query for the attention over the context.\n",
    "  x = self.attention(x, context)\n",
    "  self.last_attention_weights = self.attention.last_attention_weights\n",
    "  shape_checker(x, 'batch t units')\n",
    "  shape_checker(self.last_attention_weights, 'batch t s')\n",
    "\n",
    "  # Step 4. Generate logit predictions for the next token.\n",
    "  logits = self.output_layer(x)\n",
    "  shape_checker(logits, 'batch t target_vocab_size')\n",
    "\n",
    "  if return_state:\n",
    "    return logits, state\n",
    "  else:\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:16.611285Z",
     "iopub.status.busy": "2023-02-11T12:11:16.610286Z",
     "iopub.status.idle": "2023-02-11T12:11:16.956160Z",
     "shell.execute_reply": "2023-02-11T12:11:16.955346Z",
     "shell.execute_reply.started": "2023-02-11T12:11:16.611258Z"
    },
    "id": "i2cmV9guAFIZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(target_text_processor, UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:17.711536Z",
     "iopub.status.busy": "2023-02-11T12:11:17.710612Z",
     "iopub.status.idle": "2023-02-11T12:11:17.716145Z",
     "shell.execute_reply": "2023-02-11T12:11:17.715506Z",
     "shell.execute_reply.started": "2023-02-11T12:11:17.711510Z"
    },
    "id": "zby-aq8EAMnK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"inference\"\"\"\n",
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "  batch_size = tf.shape(context)[0]\n",
    "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "  embedded = self.embedding(start_tokens)\n",
    "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:24.005692Z",
     "iopub.status.busy": "2023-02-11T12:11:24.004584Z",
     "iopub.status.idle": "2023-02-11T12:11:24.010414Z",
     "shell.execute_reply": "2023-02-11T12:11:24.009763Z",
     "shell.execute_reply.started": "2023-02-11T12:11:24.005652Z"
    },
    "id": "VqNPrbaOARuL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "  words = self.id_to_word(tokens)\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:25.756641Z",
     "iopub.status.busy": "2023-02-11T12:11:25.755977Z",
     "iopub.status.idle": "2023-02-11T12:11:25.762521Z",
     "shell.execute_reply": "2023-02-11T12:11:25.761831Z",
     "shell.execute_reply.started": "2023-02-11T12:11:25.756616Z"
    },
    "id": "t6K0YMTnATHY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "  logits, state = self(\n",
    "    context, next_token,\n",
    "    state = state,\n",
    "    return_state=True) \n",
    "  \n",
    "  if temperature == 0.0:\n",
    "    next_token = tf.argmax(logits, axis=-1)\n",
    "  else:\n",
    "    logits = logits[:, -1, :]/temperature\n",
    "    next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "  # If a sequence produces an `end_token`, set it `done`\n",
    "  done = done | (next_token == self.end_token)\n",
    "  # Once a sequence is done it only produces 0-padding.\n",
    "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "  \n",
    "  return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:29.328951Z",
     "iopub.status.busy": "2023-02-11T12:11:29.328436Z",
     "iopub.status.idle": "2023-02-11T12:11:29.335704Z",
     "shell.execute_reply": "2023-02-11T12:11:29.334749Z",
     "shell.execute_reply.started": "2023-02-11T12:11:29.328914Z"
    },
    "id": "-aJ4NaT2AZu8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"The model\"\"\"\n",
    "class Translator(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, units,\n",
    "               context_text_processor,\n",
    "               target_text_processor):\n",
    "    super().__init__()\n",
    "    # Build the encoder and decoder\n",
    "    encoder = Encoder(context_text_processor, units)\n",
    "    decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def call(self, inputs):\n",
    "    context, x = inputs\n",
    "    context = self.encoder(context)\n",
    "    logits = self.decoder(context, x)\n",
    "\n",
    "    #TODO(b/250038731): remove this\n",
    "    try:\n",
    "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:31.397241Z",
     "iopub.status.busy": "2023-02-11T12:11:31.396318Z",
     "iopub.status.idle": "2023-02-11T12:11:31.722988Z",
     "shell.execute_reply": "2023-02-11T12:11:31.722327Z",
     "shell.execute_reply.started": "2023-02-11T12:11:31.397197Z"
    },
    "id": "BmCtaKFlAjJq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
    "# model = tf.keras.models.load_model(\"/content/drive/MyDrive/model_rnn_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:32.927411Z",
     "iopub.status.busy": "2023-02-11T12:11:32.926145Z",
     "iopub.status.idle": "2023-02-11T12:11:36.194453Z",
     "shell.execute_reply": "2023-02-11T12:11:36.193365Z",
     "shell.execute_reply.started": "2023-02-11T12:11:32.927366Z"
    },
    "id": "-efc5NO5Sn91",
    "outputId": "5678bd12-7c29-4ece-e929-680f9fca634a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape: (batch, s, units) (16, 24)\n",
      "Target tokens, shape: (batch, t) (16, 24)\n",
      "logits, shape: (batch, t, target_vocabulary_size) (16, 24, 15000)\n"
     ]
    }
   ],
   "source": [
    "logits = model((ex_context_tok, ex_tar_in))\n",
    "\n",
    "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
    "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:37.964835Z",
     "iopub.status.busy": "2023-02-11T12:11:37.964452Z",
     "iopub.status.idle": "2023-02-11T12:11:37.970781Z",
     "shell.execute_reply": "2023-02-11T12:11:37.969839Z",
     "shell.execute_reply.started": "2023-02-11T12:11:37.964811Z"
    },
    "id": "HY635CQ9Al4y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:39.222489Z",
     "iopub.status.busy": "2023-02-11T12:11:39.222143Z",
     "iopub.status.idle": "2023-02-11T12:11:39.228048Z",
     "shell.execute_reply": "2023-02-11T12:11:39.227003Z",
     "shell.execute_reply.started": "2023-02-11T12:11:39.222464Z"
    },
    "id": "kZ1HAkxFAnhg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:40.527731Z",
     "iopub.status.busy": "2023-02-11T12:11:40.527052Z",
     "iopub.status.idle": "2023-02-11T12:11:40.600706Z",
     "shell.execute_reply": "2023-02-11T12:11:40.599714Z",
     "shell.execute_reply.started": "2023-02-11T12:11:40.527704Z"
    },
    "id": "KntuM83DArkc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:44.684287Z",
     "iopub.status.busy": "2023-02-11T12:11:44.683782Z",
     "iopub.status.idle": "2023-02-11T12:11:44.695567Z",
     "shell.execute_reply": "2023-02-11T12:11:44.694580Z",
     "shell.execute_reply.started": "2023-02-11T12:11:44.684248Z"
    },
    "id": "sgXnsEYwAxJO",
    "outputId": "b5dd3677-1335-42f5-e65d-479affbb3d18",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_loss': 9.615806, 'expected_acc': 6.666666666666667e-05}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
    "\n",
    "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
    " \"expected_acc\": 1/vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:46.437139Z",
     "iopub.status.busy": "2023-02-11T12:11:46.436740Z",
     "iopub.status.idle": "2023-02-11T12:11:46.443553Z",
     "shell.execute_reply": "2023-02-11T12:11:46.442337Z",
     "shell.execute_reply.started": "2023-02-11T12:11:46.437112Z"
    },
    "id": "jiqQamqJrKiL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-11T12:11:55.562291Z",
     "iopub.status.busy": "2023-02-11T12:11:55.561905Z",
     "iopub.status.idle": "2023-02-11T12:13:51.739970Z",
     "shell.execute_reply": "2023-02-11T12:13:51.738437Z",
     "shell.execute_reply.started": "2023-02-11T12:11:55.562266Z"
    },
    "id": "0Jbx-iVDBAMm",
    "outputId": "6303e615-a22f-4d46-ffed-270e9487318c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 2.6641 - masked_acc: 0.5125 - masked_loss: 2.6641 - val_loss: 3.0184 - val_masked_acc: 0.4773 - val_masked_loss: 3.0184\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 2.6891 - masked_acc: 0.5090 - masked_loss: 2.6891 - val_loss: 3.0749 - val_masked_acc: 0.4664 - val_masked_loss: 3.0749\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 79s 397ms/step - loss: 2.6681 - masked_acc: 0.5101 - masked_loss: 2.6681 - val_loss: 3.1859 - val_masked_acc: 0.4548 - val_masked_loss: 3.1859\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 80s 398ms/step - loss: 2.6408 - masked_acc: 0.5149 - masked_loss: 2.6408 - val_loss: 2.9690 - val_masked_acc: 0.4877 - val_masked_loss: 2.9690\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 79s 396ms/step - loss: 2.7199 - masked_acc: 0.5046 - masked_loss: 2.7199 - val_loss: 3.1228 - val_masked_acc: 0.4568 - val_masked_loss: 3.1228\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 80s 398ms/step - loss: 2.6994 - masked_acc: 0.5086 - masked_loss: 2.6994 - val_loss: 2.9361 - val_masked_acc: 0.4821 - val_masked_loss: 2.9361\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 2.6950 - masked_acc: 0.5115 - masked_loss: 2.6950 - val_loss: 2.7908 - val_masked_acc: 0.5054 - val_masked_loss: 2.7908\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 80s 401ms/step - loss: 2.6829 - masked_acc: 0.5058 - masked_loss: 2.6829 - val_loss: 3.0064 - val_masked_acc: 0.4695 - val_masked_loss: 3.0064\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 78s 392ms/step - loss: 2.6827 - masked_acc: 0.5112 - masked_loss: 2.6827 - val_loss: 2.9533 - val_masked_acc: 0.4794 - val_masked_loss: 2.9533\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 78s 389ms/step - loss: 2.7059 - masked_acc: 0.5099 - masked_loss: 2.7059 - val_loss: 2.8418 - val_masked_acc: 0.4861 - val_masked_loss: 2.8418\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 77s 383ms/step - loss: 2.6461 - masked_acc: 0.5203 - masked_loss: 2.6461 - val_loss: 2.9653 - val_masked_acc: 0.4890 - val_masked_loss: 2.9653\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 78s 392ms/step - loss: 2.6697 - masked_acc: 0.5091 - masked_loss: 2.6697 - val_loss: 2.9390 - val_masked_acc: 0.4915 - val_masked_loss: 2.9390\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 2.6684 - masked_acc: 0.5081 - masked_loss: 2.6684 - val_loss: 2.8711 - val_masked_acc: 0.4893 - val_masked_loss: 2.8711\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 2.1371 - masked_acc: 0.5660 - masked_loss: 2.1382 - val_loss: 2.8728 - val_masked_acc: 0.5001 - val_masked_loss: 2.8728\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 86s 431ms/step - loss: 2.0862 - masked_acc: 0.5694 - masked_loss: 2.0862 - val_loss: 2.9631 - val_masked_acc: 0.4646 - val_masked_loss: 2.9631\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 2.0866 - masked_acc: 0.5735 - masked_loss: 2.0866 - val_loss: 2.9092 - val_masked_acc: 0.4916 - val_masked_loss: 2.9092\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 77s 386ms/step - loss: 2.1243 - masked_acc: 0.5602 - masked_loss: 2.1243 - val_loss: 2.8653 - val_masked_acc: 0.4891 - val_masked_loss: 2.8653\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 2.1517 - masked_acc: 0.5637 - masked_loss: 2.1517 - val_loss: 2.8429 - val_masked_acc: 0.5033 - val_masked_loss: 2.8429\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 79s 395ms/step - loss: 2.1770 - masked_acc: 0.5630 - masked_loss: 2.1770 - val_loss: 2.8408 - val_masked_acc: 0.5011 - val_masked_loss: 2.8408\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 77s 385ms/step - loss: 2.1709 - masked_acc: 0.5600 - masked_loss: 2.1709 - val_loss: 2.7734 - val_masked_acc: 0.4967 - val_masked_loss: 2.7734\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=20,\n",
    "    steps_per_epoch = 200,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps = 60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-08T16:14:21.304920Z",
     "iopub.status.busy": "2023-02-08T16:14:21.304161Z",
     "iopub.status.idle": "2023-02-08T16:14:22.546700Z",
     "shell.execute_reply": "2023-02-08T16:14:22.545872Z",
     "shell.execute_reply.started": "2023-02-08T16:14:21.304888Z"
    },
    "id": "kq_IIHbGS-Dj",
    "outputId": "3f8d0316-8b24-43e5-d97e-de803d08c638"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses, embedding_4_layer_call_fn, embedding_4_layer_call_and_return_conditional_losses, cross_attention_2_layer_call_fn while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model.save(\"/notebooks/GYAFC_Corpus/saved_models/model\",save_format='tf')\n",
    "#model.save(\"/content/drive/MyDrive/model_rnn_attention\",save_format='tf')\n",
    "model.save_weights('/notebooks/GYAFC_Corpus/saved_models/my_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T16:29:57.338290Z",
     "iopub.status.busy": "2023-02-08T16:29:57.337836Z",
     "iopub.status.idle": "2023-02-08T16:29:57.599537Z",
     "shell.execute_reply": "2023-02-08T16:29:57.598479Z",
     "shell.execute_reply.started": "2023-02-08T16:29:57.338259Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Custom>custom_standardizations has already been registered to <function custom_standardizations at 0x00000274E8D7E320>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [50], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#customizsed function loaded again when the model is reloaded.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m@tf\u001b[39m\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mregister_keras_serializable()\n\u001b[1;32m----> 3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_standardizations\u001b[39m(input_data):\n\u001b[0;32m      4\u001b[0m   \u001b[39m# Split accented characters.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m   text \u001b[39m=\u001b[39m tf_text\u001b[39m.\u001b[39mnormalize_utf8(input_data, \u001b[39m'\u001b[39m\u001b[39mNFKD\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m   text \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mstrings\u001b[39m.\u001b[39mlower(text)\n",
      "File \u001b[1;32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\generic_utils.py:407\u001b[0m, in \u001b[0;36mregister_keras_serializable.<locals>.decorator\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    402\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot register a class that does not have a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mget_config() method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m     )\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m registered_name \u001b[39min\u001b[39;00m _GLOBAL_CUSTOM_OBJECTS:\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mregistered_name\u001b[39m}\u001b[39;00m\u001b[39m has already been registered to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_GLOBAL_CUSTOM_OBJECTS[registered_name]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m     )\n\u001b[0;32m    412\u001b[0m \u001b[39mif\u001b[39;00m arg \u001b[39min\u001b[39;00m _GLOBAL_CUSTOM_NAMES:\n\u001b[0;32m    413\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    414\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m has already been registered to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_GLOBAL_CUSTOM_NAMES[arg]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Custom>custom_standardizations has already been registered to <function custom_standardizations at 0x00000274E8D7E320>"
     ]
    }
   ],
   "source": [
    "#customizsed function loaded again when the model is reloaded.\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def custom_standardizations(input_data):\n",
    "  # Split accented characters.\n",
    "  text = tf_text.normalize_utf8(input_data, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  return tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "#model.load_weights('/notebooks/GYAFC_Corpus/saved_models/my_weights')\n",
    "#model = tf.keras.models.load_model('path/to/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "VZSb5mFE1mXV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "def translate(self,\n",
    "              texts, *,\n",
    "              max_length=50,\n",
    "              temperature=0.0):\n",
    "  # Process the input texts\n",
    "  context = self.encoder.convert_input(texts)\n",
    "  batch_size = tf.shape(texts)[0]\n",
    "\n",
    "  # Setup the loop inputs\n",
    "  tokens = []\n",
    "  attention_weights = []\n",
    "  next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    # Generate the next token\n",
    "    next_token, done, state = self.decoder.get_next_token(\n",
    "        context, next_token, done,  state, temperature)\n",
    "        \n",
    "    # Collect the generated tokens\n",
    "    tokens.append(next_token)\n",
    "    attention_weights.append(self.decoder.last_attention_weights)\n",
    "    \n",
    "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "      break\n",
    "\n",
    "  # Stack the lists of tokens and attention weights.\n",
    "  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "  result = self.decoder.tokens_to_text(tokens)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "tnz9DJZe1s6p",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yo so stupid , im not stupid . . . never gonna give you up . '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.translate([\"Your mother is unintelligent. I am not stupid. Never gonna give you up\"]) # Are you still home\n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "zJeGpkyr1s7v",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no sorry all i know is that it was a good movie '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.translate([\"No, I apologize; all I know is that it was a good movie\"]) # Are you still home\n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'depends on how much money and psp you have '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.translate([\"It depends on how much money and drugs you have\"]) # Are you still home\n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8bba4f2be892cbd4f39a9f240acc2c7ebb8cef7264d56c87dbb8c6b0bcbe231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
