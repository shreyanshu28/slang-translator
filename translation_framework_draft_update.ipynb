{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c388e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Please keep in mind that this whole document is only\n",
    "to give a general idea of what to implement. \n",
    "These are not functioning codes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d30e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce41787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61d564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8684d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1acc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3f9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17772a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438e3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft : implementation step\n",
    "######################################\n",
    "# translate regular sentence to slang#\n",
    "######################################\n",
    "\n",
    "input_str = \"Thanks for taking the time to post. It appears the making of a nation is missing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7b81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_doc = nlp(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d412e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4daa32f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thanks, the time, It, the making, a nation]\n"
     ]
    }
   ],
   "source": [
    "print(str(list(str_doc.noun_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e631bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_phrases_extract(nlp_result):\n",
    "    # NOTE: the results of spacy for eg\n",
    "    # .noun_chunks is constituted of elements \n",
    "    # rather than simple strings \n",
    "    noun_lst = nlp_result.noun_chunks\n",
    "    return list(noun_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c91386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_phrases_extract(nlp_result):\n",
    "    # under a different scheme\n",
    "    # acknowledgement: \n",
    "    # https://stackoverflow.com/questions/47856247/extract-verb-phrases-using-spacy\n",
    "    pattern = [[{'POS': 'VERB', 'OP': '?'},{'POS': 'ADV', 'OP': '*'}, # additional wildcard - match any text in between\n",
    "               {'POS': 'VERB', 'OP': '+'}]]\n",
    "    print(type(pattern))\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", pattern)\n",
    "    matches = matcher(nlp_result)\n",
    "    spans = [nlp_result[start:end] for _, start, end in matches]\n",
    "    return filter_spans(spans)\n",
    "    # preserve the parsed positions of words so that the -ing forms\n",
    "    # won't get messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e88e96c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I, need, a break]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases_extract(nlp(\"I am really in need of a break.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da42841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" kopfschmerzen: how to translate \"I am really in need of a break\" to \n",
    "\" I rnab\" for eg if rnab=really need a break is some kind of legal slang. \n",
    "IDK if fuzzy search can do that and I hope urban dic has abbreviations 2\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98239da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: not finished.\n",
    "def phrase_extraction(nlp_result):\n",
    "    # lemmatize first, extract phrases later\n",
    "    lst_sentence = []\n",
    "    for token in nlp_result:\n",
    "        print(token.pos_)\n",
    "        word = token.lemma_\n",
    "        lst_sentence.append(word)\n",
    "    sentence = \" \".join(lst_sentence)\n",
    "    nlp_result = nlp(sentence)\n",
    "    print(nlp_result)\n",
    "    noun_list = noun_phrases_extract(nlp_result)\n",
    "    verb_list = verb_phrases_extract(nlp_result)\n",
    "    lst_phrase_temp = noun_list+verb_list\n",
    "    return lst_phrase_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7d04fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "ADP\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "PART\n",
      "VERB\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "AUX\n",
      "VERB\n",
      "PUNCT\n",
      "thank for take the time to post . it appear the making of a nation be miss .\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[the time, it, the making, a nation, thank, take, post, appear, miss]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_extraction(str_doc)\n",
    "# doing an adequate job but can be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dad3262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I love\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64af0c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'taking'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_phrases_extract(str_doc)[0]\n",
    "# NOTE: theres room for improvement in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea59a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To find and return subject of a preprocessed sentence.\"\"\"\n",
    "# I wrote the function based on which one may\n",
    "# omit the subject of a sentence with a possibility. \n",
    "# This is not used now, as I still don't\n",
    "# know how to calculate the weight of such omission.\n",
    "def subject_find(doc):\n",
    "    for token in doc:\n",
    "        if \"subj\" in token.dep_:\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do slang dictionary query with part of the input sentence\n",
    "\"\"\"returns all possible k-word-shingle for the input sentence.\n",
    "form: list, [[1-shingle][2-shingle]...[k-shingle]]\"\"\" \n",
    "def multiple_shingle(doc_str):\n",
    "    doc = doc_str.split(\" \")\n",
    "    shingle_lst=[]\n",
    "    for k in range(0,len(doc)):\n",
    "        shingle_lst_temp = []\n",
    "        for i in range(len(doc)-k):\n",
    "            shingle_lst_temp.append(doc[i:k+i+1])\n",
    "        shingle_lst.append(shingle_lst_temp)\n",
    "    return shingle_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4259fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['I'], ['have'], ['something'], ['to'], ['say']],\n",
       " [['I', 'have'], ['have', 'something'], ['something', 'to'], ['to', 'say']],\n",
       " [['I', 'have', 'something'],\n",
       "  ['have', 'something', 'to'],\n",
       "  ['something', 'to', 'say']],\n",
       " [['I', 'have', 'something', 'to'], ['have', 'something', 'to', 'say']],\n",
       " [['I', 'have', 'something', 'to', 'say']]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_shingle(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perform lemmatization to all the shingles with POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cd8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ES search function on searching abbreviation of phrases\n",
    "\"\"\" \n",
    "\"\"\"\n",
    "def search_es_abbreviation(list_words):\n",
    "    return abbreviation->str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"search in abbreviation database based on shingles,\n",
    "return slang and the corresponding shingle.\"\"\"\n",
    "\n",
    "def abbreviation_search(shingle_lst):\n",
    "    lst_search_result = []\n",
    "    for i in range(len(shingle_lst)):\n",
    "        list_temp = shingle_lst[i]\n",
    "        for j in range(len(list_temp)):\n",
    "            word_temp = list_temp[j]\n",
    "            search_result = search_es_abbreviation(word_temp)\n",
    "            # TODO: make it a fuzzy search\n",
    "            if search_result: # when search result non empty\n",
    "                # give list of paired (word, search result)\n",
    "                lst_search_result.append((word_temp, search_result))\n",
    "    return lst_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" use Urban dictionary to perform fuzzy search.\n",
    "TODO\n",
    "NOTE: another possible way is to bring the word to the phrase,\n",
    "namely extract keyword from urban dictionary&match it with the phrase.\"\"\"\n",
    "def UD_slang_search(phrase_list):\n",
    "    return slang_list\n",
    "    # formulation of slang_list: [(slang, word/phrase)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"search in UD database based on phrases,\n",
    "return slang and the corresponding phrase.\"\"\"\n",
    "\n",
    "def slang_search(phrase_lst):\n",
    "    lst_search_result = []\n",
    "    for i in range(len(phrase_lst)):\n",
    "        list_temp = phrase_lst[i]\n",
    "        for j in range(len(list_temp)):\n",
    "            word_temp = list_temp[j]\n",
    "            search_result = UD_slang_search(word_temp)\n",
    "            # TODO: make it a fuzzy search\n",
    "            if search_result: # when search result non empty\n",
    "                # give list of paired (word, search result)\n",
    "                lst_search_result.append((word_temp, search_result))\n",
    "    return lst_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5da7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ES search in emoji database and return \n",
    "# list of corresponding emojis\n",
    "def emoji_search_es(word):\n",
    "    return emoji_list->list\n",
    "    # return a list of emoji searched\n",
    "    # can be UTF-8 encoded or numbered according to order in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "214dc388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"remove stop words,\n",
    "perform a word association based on twitter corpus.\n",
    "perform ES query based on associated words.\n",
    "return: corresponding word and associated emoji as list of tuple.\"\"\"\n",
    "\n",
    "\n",
    "def emoji_search(input_str):\n",
    "    emoji_search_results = []\n",
    "    en_stops = nltk.corpus.stopwords.words(\"english\")\n",
    "    word_list = [x.strip() for x in input_str.split(\" \")]\n",
    "    model = Word2Vec.load('model.bin')\n",
    "    for word in word_list:\n",
    "        emoji_search_temp = []\n",
    "        if word not in en_stops:\n",
    "            # perform association\n",
    "            # perform emoji search\n",
    "            list_of_emoji_original = emoji_search_es([word])\n",
    "            emoji_search_temp = emoji_search_temp + list_of_emoji_original\n",
    "            if not len(list_of_emoji_original):\n",
    "                # if the origial word cannot be found\n",
    "                association_list = model.wv.most_similar(word)\n",
    "                for word_associated in association_list:\n",
    "                # store in list_of_emoji\n",
    "                    list_of_emoji = emoji_search_es(word_associated)\n",
    "                    emoji_search_temp = emoji_search_temp + list_of_emoji\n",
    "        emoji_search_results.append((word, emoji_search_temp))\n",
    "    return emoji_search_results\n",
    "    # TODO: can also based on api to reversedictionary.com\n",
    "    # which gives associated words to noun phrases: \n",
    "    # 1. extract noun phrases with nlp().noun_chunks,2.search\n",
    "    # the phrase up in reversedictionary.com, and\n",
    "    # 3.perform query with ES in emoji dataset with obtained \n",
    "    # set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d100d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_search_alternative(word_lst):\n",
    "    emoji_search_results = []\n",
    "    for word in word_lst:\n",
    "        emoji_search_temp = []\n",
    "            # perform association\n",
    "        association_list = model.wv.most_similar(word)\n",
    "            # perform emoji search\n",
    "        for word_associated in association_list:\n",
    "            # store in list_of_emoji\n",
    "            list_of_emoji = emoji_search_es(word_associated)\n",
    "        # TODO: return word_associated with associated emoji\n",
    "            emoji_search_temp = emoji_search_temp + list_of_emoji\n",
    "        emoji_search_results.append((word, emoji_search_temp))\n",
    "    return emoji_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The general function that would \n",
    "translate input to sentence with slang&emoji.\n",
    "return&print translated sentence.\n",
    "To be continued.\n",
    "\"\"\"\n",
    "def translation_style(input_str):\n",
    "    # for each of the words perform emoji search first\n",
    "    # if we want the precise results of emoji association,\n",
    "    # it would be best to not perform association based on trivial words\n",
    "    # -> STOP WORD REMOVAL\n",
    "    emoji_list = emoji_search(input_str)\n",
    "    # completely word based so we might just \n",
    "    # append the emoji after words.\n",
    "    # another way is to only perform association \n",
    "    # when you encounter a certain grammartical structure\n",
    "    # for eg noun phrase&verb phrase. \n",
    "    str_doc = nlp(input_str)\n",
    "    word_list = []\n",
    "    for word in str_doc:\n",
    "        if word.pos_ == \"VERB\" or word.pos_ == \"NOUN\" or word.pos_ == \"ADJ\":\n",
    "            word_list.append(word)\n",
    "    emoji_list_alternative = emoji_search_alternative(word_list)\n",
    "    # need to note the order of the words since same word\n",
    "    # can appear multiple times as multiple POS\n",
    "    phrase_list = phrase_extraction(str_doc)\n",
    "    \n",
    "    shingle_lst = multiple_shingle(input_str)\n",
    "    # the problem here is that after translating input\n",
    "    # to slangs the word formulation would differ.\n",
    "    # ->should make sure which part translates to which\n",
    "    abbrev_list = abbreviation_search(shingle_lst)\n",
    "    # search for further slangs with the original sentence\n",
    "    # and if the slang results overlap with the abbreviations\n",
    "    # for eg got 2 slangs for same word\n",
    "    # then use any one with equal probability\n",
    "    slang_list = slang_search(phrase_list)\n",
    "    # TODO: get the results together:\n",
    "    # append the emojis->\n",
    "    # see if the abbrev list clashes with the UD one->\n",
    "    # decide the final list->\n",
    "    # substitute words with slangs\n",
    "    return slang_sentence\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c690dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
