{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c388e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Please keep in mind that this whole document is only\n",
    "to give a general idea of what to implement. \n",
    "These are not functioning codes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86810ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.28.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests==2.28.1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests==2.28.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests==2.28.1) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests==2.28.1) (3.4)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 23.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"requests==2.28.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165a2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835c43af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Elasticsearch([{'host': 'localhost', 'port': 9200}])>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "sys.path.insert(1, '/Users/anureddy/Desktop/Sem01/DataScience_for_text_analytics/slang-translator/Connection')\n",
    "    \n",
    "from elastic_con import client \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d30e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce41787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fcd7bd2e910>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61d564b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w9/ksgfnxyn4jg66j5nhtx4v0mc0000gn/T/ipykernel_5455/3415016762.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8684d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1acc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c3f9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17772a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "438e3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft : implementation step\n",
    "######################################\n",
    "# translate regular sentence to slang#\n",
    "######################################\n",
    "\n",
    "#input_str = \"Thanks for taking the time to post. It appears the making of a nation is missing.\"\n",
    "input_str = 'He is very hot and sexy '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af7b81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_doc = nlp(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d412e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4daa32f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He]\n"
     ]
    }
   ],
   "source": [
    "print(str(list(str_doc.noun_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e631bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_phrases_extract(nlp_result):\n",
    "    # NOTE: the results of spacy for eg\n",
    "    # .noun_chunks is constituted of elements \n",
    "    # rather than simple strings \n",
    "    noun_lst = nlp_result.noun_chunks\n",
    "    return list(noun_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c91386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_phrases_extract(nlp_result):\n",
    "    # under a different scheme\n",
    "    # acknowledgement: \n",
    "    # https://stackoverflow.com/questions/47856247/extract-verb-phrases-using-spacy\n",
    "    pattern = [[{'POS': 'VERB', 'OP': '?'},{'POS': 'ADV', 'OP': '*'}, # additional wildcard - match any text in between\n",
    "               {'POS': 'VERB', 'OP': '+'}]]\n",
    "    print(type(pattern))\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", pattern)\n",
    "    matches = matcher(nlp_result)\n",
    "    spans = [nlp_result[start:end] for _, start, end in matches]\n",
    "    return filter_spans(spans)\n",
    "    # preserve the parsed positions of words so that the -ing forms\n",
    "    # won't get messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e88e96c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[He]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases_extract(nlp(\"He is very hot and sexy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da42841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" kopfschmerzen: how to translate \"I am really in need of a break\" to \n",
    "\" I rnab\" for eg if rnab=really need a break is some kind of legal slang. \n",
    "IDK if fuzzy search can do that and I hope urban dic has abbreviations 2\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98239da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: not finished.\n",
    "def phrase_extraction(nlp_result):\n",
    "    # lemmatize first, extract phrases later\n",
    "    lst_sentence = []\n",
    "    for token in nlp_result:\n",
    "        print(token.pos_)\n",
    "        word = token.lemma_\n",
    "        lst_sentence.append(word)\n",
    "    sentence = \" \".join(lst_sentence)\n",
    "    nlp_result = nlp(sentence)\n",
    "    print(nlp_result)\n",
    "    noun_list = noun_phrases_extract(nlp_result)\n",
    "    verb_list = verb_phrases_extract(nlp_result)\n",
    "    lst_phrase_temp = noun_list+verb_list\n",
    "    return lst_phrase_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "694bd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#st1 = 'i will laugh out really loud or i told her never ever to give up'\n",
    "st1 = \"He is very hot and sexy\"\n",
    "s1 = nlp(st1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d04fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON\n",
      "AUX\n",
      "ADV\n",
      "ADJ\n",
      "CCONJ\n",
      "ADJ\n",
      "he be very hot and sexy\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[he, be]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_extraction(s1)\n",
    "# doing an adequate job but can be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dad3262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I love\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64af0c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_phrases_extract(s1)\n",
    "# NOTE: theres room for improvement in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ea59a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To find and return subject of a preprocessed sentence.\"\"\"\n",
    "# I wrote the function based on which one may\n",
    "# omit the subject of a sentence with a possibility. \n",
    "# This is not used now, as I still don't\n",
    "# know how to calculate the weight of such omission.\n",
    "def subject_find(doc):\n",
    "    for token in doc:\n",
    "        if \"subj\" in token.dep_:\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42ed3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do slang dictionary query with part of the input sentence\n",
    "\"\"\"returns all possible k-word-shingle for the input sentence.\n",
    "form: list, [[1-shingle][2-shingle]...[k-shingle]]\"\"\" \n",
    "def multiple_shingle(doc_str):\n",
    "    doc = doc_str.split(\" \")\n",
    "    shingle_lst=[]\n",
    "    for k in range(0,len(doc)):\n",
    "        shingle_lst_temp = []\n",
    "        for i in range(len(doc)-k):\n",
    "            shingle_lst_temp.append(doc[i:k+i+1])\n",
    "        shingle_lst.append(shingle_lst_temp)\n",
    "    return shingle_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da4259fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_shingles = multiple_shingle(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "61adc656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's what i always use for music, movies and books\n"
     ]
    }
   ],
   "source": [
    "## Remove the extra lengthning of words if present:\n",
    "\n",
    "## TODO make it general and look into special use cases \n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def reduce_lengthening(token):\n",
    "#     if len(token) > 2:\n",
    "#         pattern = re.compile(r\"(.)\\1{1,}\")\n",
    "#         return pattern.sub(r\"\\1\", token.text)\n",
    "#     else:\n",
    "#         return token.text\n",
    "def remove_extra_lengthning(sentence):\n",
    "    words = sentence.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        i = 3\n",
    "        while i < len(word):\n",
    "            if word[i] == word[i-1]:\n",
    "                word = word[:i] + word[i+1:]\n",
    "            else:\n",
    "                i += 1\n",
    "        corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "\n",
    "#sentence = \"Anyways Titanic is my favorite Movie i cry all the time i see it!\"## This will be the informal sentance \n",
    "sentence = \"It's what I always use for music, movies and books\"\n",
    "sentence = sentence.lower()\n",
    "sen = remove_extra_lengthning(sentence)\n",
    "print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "1628df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's what i always use for music , movies and books\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['a', 'an', 'the','is','iss','but','in','on','to','my']\n",
    "\n",
    "# Define a function to remove specific stop words from a sentence\n",
    "def remove_stop_words(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    filtered_sentence = [token.text for token in doc if not token.is_stop or token.text not in stop_words and not token.is_punct]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Test the function on a sample sentence\n",
    "filtered_sentence = remove_stop_words(sen)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0766ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca78870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "8d64ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the common abbrevation into expanded form for giving as an input to slang convertor \n",
    "## TODO need to generalise this function \n",
    "def convert_words(input_string):\n",
    "    input_string = input_string.replace(\"do nt\", \"do not\")\n",
    "    input_string = input_string.replace(\"wo nt\", \"would not\")\n",
    "    input_string = input_string.replace(\"ca nt\", \"can not\")\n",
    "    input_string = input_string.replace(\"sha nt\", \"shall not\")\n",
    "    input_string = input_string.replace(\"ai nt\", \"am not\")\n",
    "    input_string = input_string.replace(\"ai nt't\", \"am not\")\n",
    "    input_string = input_string.replace(\"have nt\", \"have not\")\n",
    "    input_string = input_string.replace(\"has nt\", \"has not\")\n",
    "    input_string = input_string.replace(\"had nt\", \"had not\")\n",
    "    input_string = input_string.replace(\"is nt\", \"is not\")\n",
    "    input_string = input_string.replace(\"were nt\", \"were not\")\n",
    "    input_string = input_string.replace(\"are nt\", \"are not\")\n",
    "    #input_string = input_string.replace(\"'s\", \"it is\")\n",
    "    return input_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "7174c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's what i always use for music , movies and books\n"
     ]
    }
   ],
   "source": [
    "\n",
    "converted_string = convert_words(filtered_sentence)\n",
    "print(converted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "d26d6125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anureddy/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "index_name = \"slang-demo-json\"\n",
    "request = '''{\"query\": {\"match_all\": {}}}'''\n",
    "results = client.search(index=index_name, body=request,size = 100)['hits']['hits']\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5d63f22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anureddy/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:19: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n"
     ]
    }
   ],
   "source": [
    "def create_search_query(meaning_value):\n",
    "    query = '''{\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"meaning\": \"''' + meaning_value + '''\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }'''\n",
    "    return query\n",
    "\n",
    "#meaning = \"he is a sexy and hot person\"\n",
    "results = create_search_query(converted_string)\n",
    "results = client.search(index=index_name, body=results,size = 100)['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "d30e63e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaf --> always and forever\n",
      "afs --> always forever and seriously\n",
      "fwiw --> for what it is worth\n",
      "iwid --> it is what i do\n",
      "ouo --> official use only\n",
      "iiwii --> it is what it is\n",
      "gfi --> go for it\n",
      "utfl --> use the force luke\n",
      "bgm --> background music\n",
      "mtv --> music television\n",
      "ialto --> i always love that one\n",
      "iwaly --> i will always love you\n",
      "idm --> it does not matter intelligent dance music\n",
      "4eae --> for ever and ever\n",
      "wdim --> what does it mean\n",
      "wtii --> what time is it\n",
      "idi --> i doubt it\n",
      "igi --> i get it\n",
      "ili --> i love it\n",
      "amv --> anime music video\n",
      "ebm --> electronic body music\n",
      "mpa --> music publishers association\n",
      "vgm --> video game music\n",
      "vma --> video music awards\n",
      "wmg --> warner music group\n",
      "mmi --> me myself and i\n",
      "aap --> always a pleasure\n",
      "bfa --> best friends always\n",
      "lya --> love you always\n",
      "ainec --> and it is not even close\n",
      "ttjasi --> take this job and shove it\n",
      "aisi --> as i see it\n",
      "aiui --> as i understand it\n",
      "icbi --> i cannot believe it\n",
      "iwhi --> i would hit it\n",
      "anfscd --> and now for something completely different\n",
      "kwim --> know what i mean\n",
      "nwim --> not what i mean\n",
      "wcis --> what can i say\n",
      "wdic --> what do i care\n",
      "wwit --> what was i thinking\n",
      "nedm --> not even doom music\n",
      "omfug --> other music found underground\n",
      "aityd --> and i think you do\n",
      "italy --> i trust and love you\n",
      "lyaaf --> love you as a friend love you always and forever\n",
      "iitywimiwhtky --> if i tell you what it means i will have to kill you\n",
      "aayf --> as always your friend\n",
      "aimh --> always in my heart\n",
      "aktf --> always keep the faith\n",
      "atoy --> always thinking of you\n",
      "yala --> you always live again\n",
      "idbi --> i do not believe it\n",
      "idgi --> i do not get it\n",
      "idli --> i do not like it\n",
      "itai --> i will think about it\n",
      "aeae --> and ever and ever\n",
      "ikwum --> i know what you mean\n",
      "ikwym --> i know what you mean\n",
      "iswym --> i see what you mean\n",
      "kwis --> know what i am saying\n",
      "lwicd --> look what i can do\n",
      "ykwim --> you know what i mean\n",
      "wiwt --> what i wore today wish i was there\n",
      "anfawfos --> and now for a word from our sponsor\n",
      "japan --> just always pray at night\n",
      "idfli --> i do not feel like it\n",
      "istatoy --> i saw this and thought of you\n",
      "iykwim --> if you know what i mean\n",
      "iswydt --> i see what you did there\n",
      "iyswim --> if you see what i mean\n",
      "ykwis --> you know what i am saying\n",
      "icbinb --> i cannot believe it is not butter\n",
      "ioi --> indication of interest i am over it\n",
      "iitwi --> in it to win it\n",
      "idkwtd --> i do not know what to do\n",
      "idkwym --> i do not know what you mean\n",
      "ikwydls --> i know what you did last summer\n",
      "ps&qs --> pints and quarts please and thank yous\n",
      "bffae --> best friends forever and ever best friends for all eternity\n",
      "gas --> greetings and salutations\n",
      "h&k --> hugs and kisses\n",
      "xoxo --> hugs and kisses\n",
      "aak --> alive and kicking\n",
      "lak --> love and kisses\n",
      "oao --> over and out\n",
      "pnl --> peace and love\n",
      "pnp --> party and play\n",
      "rnb --> rhythm and blues\n",
      "snd --> search and destroy\n",
      "was --> wait and see\n",
      "idwtai --> i do not want to talk about it\n",
      "lf --> looking for\n",
      "bic --> believe it comrade\n",
      "gdi --> god damn it\n",
      "giy --> google it yourself\n",
      "gwi --> get with it\n",
      "hgi --> how goes it\n",
      "iad --> it all depends\n",
      "jdi --> just do it\n"
     ]
    }
   ],
   "source": [
    "for d in results:\n",
    "    print(d['_source']['acronym'],'-->',d['_source']['meaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "d71ee493",
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning = []\n",
    "acronym  = []\n",
    "for d in results:\n",
    "    meaning.append(d['_source']['meaning'])\n",
    "    acronym.append(d['_source']['acronym'])\n",
    "slang_dict = dict(zip(acronym,meaning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "98a25653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it 's what i always use for music , movies and books\""
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_slang(input_string):\n",
    "    for acronym, meaning in slang_dict.items():\n",
    "        \n",
    "        input_string = input_string.replace(meaning, acronym)\n",
    "    return input_string\n",
    "slang_sen = convert_to_slang(converted_string)\n",
    "slang_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "1f4cd8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: ES search function on searching abbreviation of phrases\n",
    "# \"\"\" \n",
    "# \"\"\"\n",
    "# def search_es_abbreviation(list_words):\n",
    "    \n",
    "#     return abbreviation->str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"search in abbreviation database based on shingles,\n",
    "# return slang and the corresponding shingle.\"\"\"\n",
    "\n",
    "# def abbreviation_search(shingle_lst):\n",
    "#     lst_search_result = []\n",
    "#     for i in range(len(shingle_lst)):\n",
    "#         list_temp = shingle_lst[i]\n",
    "#         for j in range(len(list_temp)):\n",
    "#             word_temp = list_temp[j]\n",
    "#             search_result = search_es_abbreviation(word_temp)\n",
    "#             # TODO: make it a fuzzy search\n",
    "#             if search_result: # when search result non empty\n",
    "#                 # give list of paired (word, search result)\n",
    "#                 lst_search_result.append((word_temp, search_result))\n",
    "#     return lst_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" use Urban dictionary to perform fuzzy search.\n",
    "# TODO\n",
    "# NOTE: another possible way is to bring the word to the phrase,\n",
    "# namely extract keyword from urban dictionary&match it with the phrase.\"\"\"\n",
    "# def UD_slang_search(phrase_list):\n",
    "#     return slang_list\n",
    "#     # formulation of slang_list: [(slang, word/phrase)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"search in UD database based on phrases,\n",
    "# return slang and the corresponding phrase.\"\"\"\n",
    "\n",
    "# def slang_search(phrase_lst):\n",
    "#     lst_search_result = []\n",
    "#     for i in range(len(phrase_lst)):\n",
    "#         list_temp = phrase_lst[i]\n",
    "#         for j in range(len(list_temp)):\n",
    "#             word_temp = list_temp[j]\n",
    "#             search_result = UD_slang_search(word_temp)\n",
    "#             # TODO: make it a fuzzy search\n",
    "#             if search_result: # when search result non empty\n",
    "#                 # give list of paired (word, search result)\n",
    "#                 lst_search_result.append((word_temp, search_result))\n",
    "#     return lst_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff6ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "5adce292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anureddy/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:4: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    " ##TODO need to generalise  use the better emoji dataset if possible \n",
    "index_name = \"emo31\"\n",
    "results_emo = '''{\"query\": {\"match_all\": {}}}'''\n",
    "results_emo = client.search(index=index_name, body=results_emo,size = 100)['hits']['hits']\n",
    "#print(results_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "adca8d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it 's what i always use for music , movies and books\""
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "99c48580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anureddy/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:19: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n"
     ]
    }
   ],
   "source": [
    "def create_search_query(meaning_value):\n",
    "    query = '''{\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"meaning\": \"''' + meaning_value + '''\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }'''\n",
    "    return query\n",
    "\n",
    "#meaning = \"he is a sexy and hot person\"\n",
    "results_emo = create_search_query(slang_sen)\n",
    "results_emo = client.search(index=index_name, body=results_emo,size = 100)['hits']['hits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "e626c635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😦 --> ['frowning_face_with_open_mouth', 'face', 'aw', 'what']\n",
      "📚 --> ['books', 'literature', 'library', 'study']\n",
      "😦 --> ['frowning_face_with_open_mouth', 'face', 'aw', 'what']\n",
      "📚 --> ['books', 'literature', 'library', 'study']\n",
      "😖 --> ['confounded_face', 'face', 'confused', 'sick', 'unwell', 'oops', ':S']\n",
      "😖 --> ['confounded_face', 'face', 'confused', 'sick', 'unwell', 'oops', ':S']\n",
      "🪗 --> ['accordion', 'music']\n",
      "🪗 --> ['accordion', 'music']\n",
      "📯 --> ['postal_horn', 'instrument', 'music']\n",
      "🎶 --> ['musical_notes', 'music', 'score']\n",
      "🎸 --> ['guitar', 'music', 'instrument']\n",
      "🎺 --> ['trumpet', 'music', 'brass']\n",
      "🪕 --> ['banjo', 'music', 'instructment']\n",
      "🪘 --> ['long drum', 'music']\n",
      "📯 --> ['postal_horn', 'instrument', 'music']\n",
      "🎶 --> ['musical_notes', 'music', 'score']\n",
      "🎸 --> ['guitar', 'music', 'instrument']\n",
      "🎺 --> ['trumpet', 'music', 'brass']\n",
      "🪕 --> ['banjo', 'music', 'instructment']\n",
      "🪘 --> ['long drum', 'music']\n",
      "🎧 --> ['headphone', 'music', 'score', 'gadgets']\n",
      "🎧 --> ['headphone', 'music', 'score', 'gadgets']\n",
      "📻 --> ['radio', 'communication', 'music', 'podcast', 'program']\n",
      "🎷 --> ['saxophone', 'music', 'instrument', 'jazz', 'blues']\n",
      "🎻 --> ['violin', 'music', 'instrument', 'orchestra', 'symphony']\n",
      "🥁 --> ['drum', 'music', 'instrument', 'drumsticks', 'snare']\n",
      "🔘 --> ['radio_button', 'input', 'old', 'music', 'circle']\n",
      "📻 --> ['radio', 'communication', 'music', 'podcast', 'program']\n",
      "🎷 --> ['saxophone', 'music', 'instrument', 'jazz', 'blues']\n",
      "🎻 --> ['violin', 'music', 'instrument', 'orchestra', 'symphony']\n",
      "🥁 --> ['drum', 'music', 'instrument', 'drumsticks', 'snare']\n",
      "🔘 --> ['radio_button', 'input', 'old', 'music', 'circle']\n",
      "🫰 --> ['hand with index finger and thumb crossed', 'heart', 'love', 'money', 'expensive']\n",
      "🫰 --> ['hand with index finger and thumb crossed', 'heart', 'love', 'money', 'expensive']\n",
      "🎤 --> ['microphone', 'sound', 'music', 'PA', 'sing', 'talkshow']\n",
      "🔀 --> ['shuffle_tracks_button', 'blue-square', 'shuffle', 'music', 'random']\n",
      "🎤 --> ['microphone', 'sound', 'music', 'PA', 'sing', 'talkshow']\n",
      "🔀 --> ['shuffle_tracks_button', 'blue-square', 'shuffle', 'music', 'random']\n",
      "🫢 --> ['face with open eyes and hand over mouth', 'silence', 'secret', 'shock', 'surprise']\n",
      "🫢 --> ['face with open eyes and hand over mouth', 'silence', 'secret', 'shock', 'surprise']\n",
      "🔣 --> ['input_symbols', 'blue-square', 'music', 'note', 'ampersand', 'percent', 'glyphs', 'characters']\n",
      "🔣 --> ['input_symbols', 'blue-square', 'music', 'note', 'ampersand', 'percent', 'glyphs', 'characters']\n"
     ]
    }
   ],
   "source": [
    "for d in results_emo:\n",
    "    print(d['_source']['emoji'],'-->',d['_source']['meaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "1da18e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning = []\n",
    "emoji  = []\n",
    "for d in results_emo:\n",
    "    meaning.append(d['_source']['meaning'])\n",
    "    emoji.append(d['_source']['emoji'])\n",
    "emo_dict = dict(zip(emoji,meaning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "62d34bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', \"'s\", 'what', 'i', 'always', 'use', 'for', 'music', ',', 'movies', 'and', 'books']\n",
      "it 's 😦 i always use for 🪗 , movies and 📚\n"
     ]
    }
   ],
   "source": [
    "def replace_with_emoji(sentence):\n",
    "    words = sentence.split()\n",
    "    print(words)\n",
    "    emojified = [next((emoji for emoji, meanings in emo_dict.items() if word in meanings), word) for word in words]\n",
    "    return \" \".join(emojified)\n",
    "print(replace_with_emoji(slang_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b80741f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning = results_emo[0]['_source']['meaning'][0]\n",
    "emoji = results_emo[0]['_source']['emoji']\n",
    "emo_dict = {emoji:meaning}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1f44e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'🧑': 'person'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6710ec4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is a  sexy man'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_emoji(input_string):\n",
    "    for emoji, meaning in emo_dict.items():\n",
    "        \n",
    "        input_string = input_string.replace(meaning, emoji)\n",
    "    return input_string\n",
    "slang_sen = 'idk, but he is way hot'\n",
    "slang_sen1 = 'He is a  sexy man'\n",
    "slang_sen = convert_to_emoji(slang_sen1)\n",
    "slang_sen1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "874bab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n"
     ]
    }
   ],
   "source": [
    "input_string = 'He is very hot person'\n",
    "for emoji, meaning in emo_dict.items():\n",
    "        \n",
    "    input_string = input_string.replace(meaning, emoji)\n",
    "print(meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5da7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ES search in emoji database and return \n",
    "# list of corresponding emojis\n",
    "def emoji_search_es(word):\n",
    "    return emoji_list->list\n",
    "    # return a list of emoji searched\n",
    "    # can be UTF-8 encoded or numbered according to order in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "214dc388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"remove stop words,\n",
    "perform a word association based on twitter corpus.\n",
    "perform ES query based on associated words.\n",
    "return: corresponding word and associated emoji as list of tuple.\"\"\"\n",
    "\n",
    "\n",
    "def emoji_search(input_str):\n",
    "    emoji_search_results = []\n",
    "    en_stops = nltk.corpus.stopwords.words(\"english\")\n",
    "    word_list = [x.strip() for x in input_str.split(\" \")]\n",
    "    model = Word2Vec.load('model.bin')\n",
    "    for word in word_list:\n",
    "        emoji_search_temp = []\n",
    "        if word not in en_stops:\n",
    "            # perform association\n",
    "            # perform emoji search\n",
    "            list_of_emoji_original = emoji_search_es([word])\n",
    "            emoji_search_temp = emoji_search_temp + list_of_emoji_original\n",
    "            if not len(list_of_emoji_original):\n",
    "                # if the origial word cannot be found\n",
    "                association_list = model.wv.most_similar(word)\n",
    "                for word_associated in association_list:\n",
    "                # store in list_of_emoji\n",
    "                    list_of_emoji = emoji_search_es(word_associated)\n",
    "                    emoji_search_temp = emoji_search_temp + list_of_emoji\n",
    "        emoji_search_results.append((word, emoji_search_temp))\n",
    "    return emoji_search_results\n",
    "    # TODO: can also based on api to reversedictionary.com\n",
    "    # which gives associated words to noun phrases: \n",
    "    # 1. extract noun phrases with nlp().noun_chunks,2.search\n",
    "    # the phrase up in reversedictionary.com, and\n",
    "    # 3.perform query with ES in emoji dataset with obtained \n",
    "    # set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d100d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_search_alternative(word_lst):\n",
    "    emoji_search_results = []\n",
    "    for word in word_lst:\n",
    "        emoji_search_temp = []\n",
    "            # perform association\n",
    "        association_list = model.wv.most_similar(word)\n",
    "            # perform emoji search\n",
    "        for word_associated in association_list:\n",
    "            # store in list_of_emoji\n",
    "            list_of_emoji = emoji_search_es(word_associated)\n",
    "        # TODO: return word_associated with associated emoji\n",
    "            emoji_search_temp = emoji_search_temp + list_of_emoji\n",
    "        emoji_search_results.append((word, emoji_search_temp))\n",
    "    return emoji_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The general function that would \n",
    "translate input to sentence with slang&emoji.\n",
    "return&print translated sentence.\n",
    "To be continued.\n",
    "\"\"\"\n",
    "def translation_style(input_str):\n",
    "    # for each of the words perform emoji search first\n",
    "    # if we want the precise results of emoji association,\n",
    "    # it would be best to not perform association based on trivial words\n",
    "    # -> STOP WORD REMOVAL\n",
    "    emoji_list = emoji_search(input_str)\n",
    "    # completely word based so we might just \n",
    "    # append the emoji after words.\n",
    "    # another way is to only perform association \n",
    "    # when you encounter a certain grammartical structure\n",
    "    # for eg noun phrase&verb phrase. \n",
    "    str_doc = nlp(input_str)\n",
    "    word_list = []\n",
    "    for word in str_doc:\n",
    "        if word.pos_ == \"VERB\" or word.pos_ == \"NOUN\" or word.pos_ == \"ADJ\":\n",
    "            word_list.append(word)\n",
    "    emoji_list_alternative = emoji_search_alternative(word_list)\n",
    "    # need to note the order of the words since same word\n",
    "    # can appear multiple times as multiple POS\n",
    "    phrase_list = phrase_extraction(str_doc)\n",
    "    \n",
    "    shingle_lst = multiple_shingle(input_str)\n",
    "    # the problem here is that after translating input\n",
    "    # to slangs the word formulation would differ.\n",
    "    # ->should make sure which part translates to which\n",
    "    abbrev_list = abbreviation_search(shingle_lst)\n",
    "    # search for further slangs with the original sentence\n",
    "    # and if the slang results overlap with the abbreviations\n",
    "    # for eg got 2 slangs for same word\n",
    "    # then use any one with equal probability\n",
    "    slang_list = slang_search(phrase_list)\n",
    "    # TODO: get the results together:\n",
    "    # append the emojis->\n",
    "    # see if the abbrev list clashes with the UD one->\n",
    "    # decide the final list->\n",
    "    # substitute words with slangs\n",
    "    return slang_sentence\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c690dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
